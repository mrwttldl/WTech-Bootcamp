{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments for \"What is Exploratory Data Analysis?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.What is the purpose the data exploration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis is the first building block of the data science pipeline. Before moving on to the modeling phase,\n",
    "we need to fully understand the data we work with. The better we understand our data, the more useful we can identify and \n",
    "extract the most useful features, which boost the performance of the model.The main tools we use at this stage are statistics and visualization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Suppose you are working on a dataset containing customer reviews of an e-commerce company's products. Customer reviews collected through the company's website are rated between 1 and 5 depending on whether the content is positive or negative.\n",
    "\n",
    "-What kind of problems do you expect to encounter in this raw data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data is dirty. It may contain unnecessary and outliers. Some data may be missing or have duplicate data.\n",
    "Duplicate data is often dangerous. This data may be due to error or due to multiple actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-If your task is to identify the characteristics that reveal that the customers' comments are positive or negative, \n",
    "how would you do that and what methods would you use for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the right decision, our data must be clean. Secondly, we need to recognize the data, see what it means to us. Therefore, we make visualization. We use machine learning algorithms for the most important part of decision-making. We can be sure that the models we have created are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-What are the useful features that can be extracted from raw data? How can you access this data and understand whether it is useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the size of the data we have is large enough for us to research, we can draw many conclusions from the raw data. We can pull the data from the web by web scraping. We can use model evaluation methods to evaluate and interpret the results of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Why do you think missing values should be taking care of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be missing values for critical observations in the data. This can reduce the number of data points we can work on, which means we may lose some valuable information. So we take care of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Do you think that outliers have an impact on a dataset? If so, how would you explain this impact?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data sets may have outliers. These values can damage our analysis as they can affect our results. When we visualize the data and look at the distribution of values, normal data have a uniformity, while outliers can cause deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Please briefly summarize you first actions when you start analyzing the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data science project begins with researching data such that over 80% of the time a data scientist's work is devoted to this part. Before moving on to the modeling phase, we need to fully understand the data we are working on. The better we understand our data, the more useful we can identify and extract the most useful features that improve the model's performance. We have a three-step process:\n",
    "\n",
    "Data cleaning \n",
    "\n",
    "Removing unnecessary data and outliers.\n",
    "Remove duplicates\n",
    "Filling in missing values.\n",
    "\n",
    "Exploring data\n",
    "\n",
    "This second step is to investigate the data to discover relationships between different properties and patterns in the data. The main tools we use at this stage are statistics and visualization techniques.\n",
    "\n",
    "Feature engineering\n",
    "\n",
    "The final step is feature engineering, where we select the most useful features or create new features from existing features. This stage takes us to the next steps of the data science pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
